# MAKERS: Multi Agent Knowledge Exploration & Retrieval System

[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸš€ Overview

**MAKERS** is an advanced autonomous research system that combines Large Language Models (LLMs) with strategic tool orchestration. It leverages **LangGraph** for workflow orchestration, **LlamaIndex** for Retrieval Augmented Generation (RAG), **ChromaDB** for vector storage, and **SQLite** for persistent state management (LangGraph checkpoints).

### Core Capabilities

*   **Autonomous Research Agent**: Unified ReAct agent that dynamically orchestrates multiple information sources
*   **Multi-Source Retrieval**: Intelligent decision-making between ArXiv search (external) and knowledge base RAG (internal)
*   **Deep Document Analysis**: Specialized CrewAI team for comprehensive PDF analysis
*   **Stateful Workflows**: Persistent, resumable research sessions with SQLite checkpointing
*   **Long-Term Memory**: Conversation summarization prevents prompt explosion while preserving context

## ğŸ—ï¸ Architecture

### System Architecture

The system implements a **multi-node LangGraph workflow** with separated concerns for agent reasoning, tool execution, and memory management:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      USER REQUEST (CLI/API)                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                      â”‚
                                      â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    LANGGRAPH WORKFLOW                                     â•‘
â•‘         StateGraph with SQLite Checkpointing                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â•‘
â•‘  â•‘                      AGENT NODE                                     â•‘  â•‘
â•‘  â•‘                    (Entry Point)                                    â•‘  â•‘
â•‘  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  â•‘
â•‘  â•‘    Input:                                                           â•‘  â•‘
â•‘  â•‘     â€¢ conversation_summary (long-term memory)                       â•‘  â•‘
â•‘  â•‘     â€¢ recent messages (last 3, immediate context)                   â•‘  â•‘
â•‘  â•‘                                                                     â•‘  â•‘
â•‘  â•‘    Process:                                                         â•‘  â•‘
â•‘  â•‘     â€¢ ReAct Agent analyzes context                                  â•‘  â•‘
â•‘  â•‘     â€¢ Decision: tool_calls OR final_answer                          â•‘  â•‘
â•‘  â•‘                                                                     â•‘  â•‘
â•‘  â•‘    Output:                                                          â•‘  â•‘
â•‘  â•‘     â€¢ AIMessage with tool_calls[] OR content (final answer)         â•‘  â•‘
â•‘  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â•‘
â•‘                              â”‚                                            â•‘
â•‘                              â”‚ route_after_agent                          â•‘
â•‘                              â”‚                                            â•‘
â•‘              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â•‘
â•‘              â”‚                               â”‚                            â•‘
â•‘              â–¼                               â–¼                            â•‘
â•‘  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—      â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                 â•‘
â•‘  â•‘      TOOL NODE        â•‘      â•‘         END           â•‘                 â•‘
â•‘  â•‘   (if tool_calls)     â•‘      â•‘   (if final_answer)   â•‘                 â•‘
â•‘  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£      â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£                 â•‘
â•‘  â•‘  â€¢ Extract tool_calls â•‘      â•‘  â€¢ Return final_state â•‘                 â•‘
â•‘  â•‘  â€¢ Get from Registry  â•‘      â•‘  â€¢ Output: final_     â•‘                 â•‘
â•‘  â•‘  â€¢ Execute tools:     â•‘      â•‘    output             â•‘                 â•‘
â•‘  â•‘    - arxiv_search     â•‘      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                 â•‘
â•‘  â•‘    - knowledge_base   â•‘                                                â•‘
â•‘  â•‘    - document_analysisâ•‘                                                â•‘
â•‘  â•‘  â€¢ Return ToolMessage â•‘                                                â•‘
â•‘  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                â•‘
â•‘              â”‚                                                            â•‘
â•‘              â”‚ route_after_tool                                           â•‘
â•‘              â”‚                                                            â•‘
â•‘              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â•‘
â•‘                          â”‚               â”‚                                â•‘
â•‘                          â–¼               â–¼                                â•‘
â•‘          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             â•‘
â•‘          â•‘     SUMMARY NODE      â•‘  â•‘      AGENT NODE       â•‘             â•‘
â•‘          â•‘   (if msg_count â‰¥ 20) â•‘  â•‘   (if msg_count < 20) â•‘             â•‘
â•‘          â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£             â•‘
â•‘          â•‘  â€¢ Take: messages +   â•‘  â•‘  â€¢ Continue workflow  â•‘             â•‘
â•‘          â•‘    existing summary   â•‘  â•‘  â€¢ Process tool       â•‘             â•‘
â•‘          â•‘  â€¢ Generate condensed â•‘  â•‘    results            â•‘             â•‘
â•‘          â•‘    summary            â•‘  â•‘                       â•‘             â•‘
â•‘          â•‘  â€¢ Preserve findings  â•‘  â•‘                       â•‘             â•‘
â•‘          â•‘  â€¢ Clear old msgs     â•‘  â•‘                       â•‘             â•‘
â•‘          â•‘  â€¢ Keep last 3 msgs   â•‘  â•‘                       â•‘             â•‘
â•‘          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•             â•‘
â•‘                      â”‚                                                    â•‘
â•‘                      â”‚ route_after_summary                                â•‘
â•‘                      â”‚                                                    â•‘
â•‘                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬                                        â•‘
â•‘                                  â”‚                                        â•‘
â•‘                                  â–¼                                        â•‘
â•‘                          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                                â•‘
â•‘                          â•‘   AGENT NODE  â•‘                                â•‘
â•‘                          â•‘  (Loop back)  â•‘                                â•‘
â•‘                          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                      â”‚
                                      â–¼
                          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                          â•‘      FINAL OUTPUT     â•‘
                          â•‘  GraphState with      â•‘
                          â•‘  final_output         â•‘
                          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Key Components

1. **Agent Node** (`agentic/workflow/nodes/agent_node.py`):
   - **Input**: `conversation_summary` (string, long-term memory) + `messages` (list, recent context)
   - **Process**: ReAct agent (LangChain) analyzes context and decides on action
   - **Output**: `AIMessage` with `tool_calls` (list) OR `content` (final answer string)
   - **State Updates**: `messages`, `next_action`, `final_output`, `iteration_count`
   - **Safety**: Maximum iteration limit (50) to prevent infinite loops

2. **Tool Node** (`agentic/workflow/nodes/tool_node.py`):
   - **Input**: `messages` (extracts `tool_calls` from last `AIMessage`)
   - **Process**: Retrieves tools from `ToolRegistry`, executes each tool call
   - **Tools Available**: `arxiv_search_tool`, `knowledge_base_retrieval_tool`
   - **Note**: `document_deep_dive_analysis_tool` is not used for security reasons (prevents automatic PDF downloads)
   - **Output**: `ToolMessage` list with execution results
   - **Error Handling**: Graceful failure with error messages in `ToolMessage`
   - **Next Step**: Always routes to either `summarize` (if message_count >= 20) or `agent` (continue)

3. **Summary Node** (`agentic/workflow/nodes/summary_node.py`):
   - **Trigger**: Only after `tool_node` when `len(messages) >= SUMMARY_THRESHOLD` (20 messages)
   - **Input**: Recent messages + existing `conversation_summary`
   - **Process**: LLM-based summarization (temperature: 0.1 for factual accuracy)
   - **Output**: Condensed summary preserving key findings
   - **Memory Management**: Clears old messages, keeps last 3 for immediate context
   - **Next Step**: Always routes back to `agent` with updated summary

4. **Routing Logic** (`agentic/workflow/routing.py`):
   - **route_after_agent**: Routes to `tool` (if tool_calls), `continue` (if unclear), or `end` (if final_answer)
   - **route_after_tool**: Routes to `summarize` (if message_count >= 20) or `agent` (if message_count < 20)
   - **route_after_summary**: Always routes back to `agent` with updated summary
   - **Note**: Summary node can only be reached after tool node execution, never in parallel

5. **State Management** (`core/state.py`):
   - **GraphState**: TypedDict with `messages`, `conversation_summary`, `user_query`, `final_output`, `next_action`, `error_message`, `iteration_count`
   - **Checkpointing**: SQLite-based persistence via `services/storage/checkpointer.py`
   - **Resumability**: Thread-based state recovery for long-running sessions

### Tool Registry Architecture

The system uses a **modular Tool Registry** pattern:
- Tools are registered in `src/agentic/tools/registry.py`
- `tool_node.py` retrieves tools dynamically (no hardcoding)
- Easy to add/remove tools without modifying workflow code

### Information Retrieval Strategy

**Hybrid Approach:**
- **ArXiv Search**: Access to latest scientific papers (external)
- **Knowledge Base RAG**: Fast retrieval from curated documents (internal)
- **Intelligent Fusion**: Automatic deduplication and relevance-based merging

**Decision Logic:**
- Recent/emerging topics â†’ Prioritize ArXiv
- Established concepts â†’ Start with knowledge base
- Comprehensive research â†’ Use both sources intelligently

## ğŸ› ï¸ Tech Stack

- **LLM Orchestration**: LangGraph
- **Agent Framework**: LangChain (ReAct pattern)
- **Specialized Analysis**: CrewAI (two-agent complementary architecture)
- **RAG & Indexing**: LlamaIndex
- **Vector Database**: ChromaDB (local, with automatic cosine similarity indexing)
- **State Management**: SQLite (for LangGraph checkpoints, local, no server required)
- **LLM Providers**: OpenAI, Hugging Face, Ollama, Groq, Google Gemini (centralized factory)
- **Embedding Providers**: HuggingFace (default, local/unlimited/free), Ollama (local), OpenAI (API)
- **API**: FastAPI, Uvicorn
- **Experiment Tracking**: Weights & Biases

## ğŸ“ Directory Structure

```
src/
â”œâ”€â”€ services/        # Reusable technical services
â”‚   â”œâ”€â”€ llm.py       # LLM factory (OpenAI, HuggingFace, Ollama, Groq, Google Gemini)
â”‚   â”œâ”€â”€ storage/     # ChromaDB (vector store), SQLite (checkpoints), Checkpointer
â”‚   â”œâ”€â”€ ingestion/   # Data ingestion pipeline
â”‚   â””â”€â”€ evaluation/  # Evaluation services (RAG, synthesis)
â”œâ”€â”€ agentic/         # Agentic system
â”‚   â”œâ”€â”€ agents/      # Agents, prompts, and constants
â”‚   â”œâ”€â”€ tools/       # Tools with registry pattern
â”‚   â””â”€â”€ workflow/    # LangGraph workflow (graph, runner, nodes, routing, state, constants)
â””â”€â”€ application/     # User interfaces
    â”œâ”€â”€ api/         # FastAPI REST API
    â””â”€â”€ cli/         # Command-line interface scripts
```

## âš™ï¸ Installation & Setup

### Prerequisites

- **Python 3.11+**
- **Poetry**: Dependency management (required)
- **Groq API Key** (default): Get your free API key from [console.groq.com](https://console.groq.com)

### Installation

1. **Clone repository**:
   ```bash
   git clone https://github.com/estephe-arnaud/makers
   cd makers
   ```

2. **Configure environment**:
   ```bash
   cp .env.example .env
   ```
   
          Edit `.env`:
          ```env
          # LLM Provider (default: Groq - unlimited/free tier)
          DEFAULT_LLM_MODEL_PROVIDER=groq
          GROQ_API_KEY=your_groq_api_key_here
          GROQ_MODEL_NAME=llama-3.3-70b-versatile
          
          # Embedding Provider (default: HuggingFace - local, unlimited, free)
          DEFAULT_EMBEDDING_PROVIDER=huggingface
          HUGGINGFACE_EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
          
          # Alternative LLM Providers:
          # DEFAULT_LLM_MODEL_PROVIDER=google
          # GOOGLE_API_KEY=your_google_api_key_here
          # GOOGLE_GEMINI_MODEL_NAME=gemini-pro
          
          # DEFAULT_LLM_MODEL_PROVIDER=openai
          # OPENAI_API_KEY=your_key_here
          
          # DEFAULT_LLM_MODEL_PROVIDER=ollama
          # OLLAMA_BASE_URL=http://localhost:11434
          
          # Alternative Embedding Providers:
          # DEFAULT_EMBEDDING_PROVIDER=ollama
          # OLLAMA_EMBEDDING_MODEL_NAME=nomic-embed-text
          # OLLAMA_BASE_URL=http://localhost:11434
          
          # DEFAULT_EMBEDDING_PROVIDER=openai
          # OPENAI_API_KEY=your_key_here
          # OPENAI_EMBEDDING_MODEL_NAME=text-embedding-3-small
          ```

3. **Install dependencies**:
   ```bash
   poetry install
   ```

4. **Set up ChromaDB (Vector Storage)**:

   ChromaDB is used for vector storage and is automatically configured. No setup required! The database is stored locally at `data/chroma_db/` by default.

   **Note**: ChromaDB automatically creates vector indexes with cosine similarity, so you get optimized vector search out of the box.

5. **Set up SQLite (for LangGraph checkpoints)**:

   SQLite is used for storing LangGraph conversation checkpoints (state management). No setup required! The database is automatically created at `data/checkpoints.sqlite` by default.

   **Note**: SQLite is a lightweight, serverless database that requires no configuration. All checkpoint data is stored locally in a single file.

6. **(Optional) Connect to Weights & Biases**:
   ```bash
   poetry run wandb login
   ```

## ğŸš€ Usage

### 1. Data Ingestion

By default, the pipeline loads PDFs from a local directory. To use local PDFs:

```bash
poetry run python -m src.application.cli.run_ingestion \
  --pdf_dir /path/to/my/pdfs
```

To download PDFs from ArXiv (requires `--download_from_arxiv`):

```bash
poetry run python -m src.application.cli.run_ingestion \
  --download_from_arxiv \
  --query "What are the latest advancements in face analysis" \
  --max_results 10
```

**Main options:**
- `--pdf_dir` (required by default): Path to a directory containing PDF files
- `--download_from_arxiv`: Enable downloading from ArXiv instead of using a local directory
- `--query`: Required with `--download_from_arxiv`, query for ArXiv search
- `--arxiv_keywords`: Optimized keywords for ArXiv (required with `--download_from_arxiv`)
- `--max_results`: Maximum number of papers to download (default: 10, only with `--download_from_arxiv`)
- `--sort_by`: Sort criterion (relevance, lastUpdatedDate, submittedDate, only with `--download_from_arxiv`)
- `--corpus_name`: Specific name for the corpus (optional)
- `--collection_name`: ChromaDB collection name (default: `arxiv_chunks`)

### 2. Run MAKERS Workflow

Submit a research query to the autonomous agent:

```bash
poetry run python -m src.application.cli.run_makers \
  --query "What are the latest advancements in face analysis"
```

**Options:**
- `--query` / `-q`: Research query (required)
- `--thread_id` / `-t`: Optional thread ID to resume a previous session
- `--log_level`: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)

### 3. Run Evaluations

Evaluate system performance:

```bash
poetry run python -m src.application.cli.run_evaluation \
  --eval_type all \
  --rag_dataset data/evaluation/rag_eval_dataset.json
```

**Options:**
- `--eval_type`: Type of evaluation (rag, synthesis, all)
- `--rag_dataset`: Path to RAG evaluation dataset
- `--synthesis_dataset`: Path to synthesis evaluation dataset
- `--wandb_project`: W&B project name (default: MAKERS-Evaluations)

### 4. API Server

Start the FastAPI server:

```bash
poetry run uvicorn src.application.api.main:app --reload --host 127.0.0.1 --port 8000
```

Access:
- API: `http://localhost:8000`
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

**Example API request:**
```bash
curl -X POST "http://localhost:8000/invoke_makers" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the latest advancements in face analysis"}'
```

## ğŸ³ Docker

Build and run with Docker:

```bash
# Build image
docker build -t makers-app .

# Run CLI
docker run -e OPENAI_API_KEY=$OPENAI_API_KEY \
           makers-app \
           python -m src.application.cli.run_makers --query "What are the latest advancements in face analysis"

# Run API server
docker run -p 127.0.0.1:8000:8000 \
           -e OPENAI_API_KEY=$OPENAI_API_KEY \
           makers-app \
           uvicorn src.application.api.main:app --host 0.0.0.0 --port 8000
```

## ğŸ“„ License

MIT License - Copyright (c) 2025 EstÃ¨phe ARNAUD

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

