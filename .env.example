# =============================================================================
# MAKERS Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your API keys and configuration values.
# All settings can be overridden via environment variables.
# =============================================================================

# -----------------------------------------------------------------------------
# General Project Settings
# -----------------------------------------------------------------------------
DEBUG=false
PYTHON_ENV=development

# -----------------------------------------------------------------------------
# API Keys & Authentication
# -----------------------------------------------------------------------------
# At minimum, you need one LLM provider API key (Groq recommended for free tier)

# Groq API Key (Recommended - Free tier with generous limits)
# Get your key from: https://console.groq.com
GROQ_API_KEY=your_groq_api_key_here

# OpenAI API Key
# Get your key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=your_openai_api_key_here

# Google Gemini API Key
# Get your key from: https://makersuite.google.com/app/apikey
# GOOGLE_API_KEY=your_google_api_key_here

# Anthropic Claude API Key
# Get your key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# HuggingFace API Key (Optional - only needed for private models)
# Get your key from: https://huggingface.co/settings/tokens
# HUGGINGFACE_API_KEY=your_huggingface_api_key_here

# Tavily API Key (Optional - for web search)
# Get your key from: https://tavily.com/
# TAVILY_API_KEY=your_tavily_api_key_here

# Weights & Biases API Key (Optional - for experiment tracking)
# Get your key from: https://wandb.ai/authorize
# WANDB_API_KEY=your_wandb_api_key_here

# -----------------------------------------------------------------------------
# LLM Provider Configuration
# -----------------------------------------------------------------------------
# Default LLM provider (options: groq, openai, ollama, huggingface, google)
DEFAULT_LLM_MODEL_PROVIDER=groq

# OpenAI Configuration
DEFAULT_OPENAI_GENERATIVE_MODEL=gpt-4

# Groq Configuration
GROQ_MODEL_NAME=llama-3.3-70b-versatile

# Google Gemini Configuration
GOOGLE_GEMINI_MODEL_NAME=gemini-pro

# HuggingFace Configuration
HUGGINGFACE_REPO_ID=mistralai/Mixtral-8x7B-Instruct-v0.1

# Ollama Configuration (for local models)
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_GENERATIVE_MODEL_NAME=mistral

# -----------------------------------------------------------------------------
# Embedding Configuration
# -----------------------------------------------------------------------------
# Default embedding provider (options: huggingface, openai, ollama)
# HuggingFace is recommended: local, unlimited, free
DEFAULT_EMBEDDING_PROVIDER=huggingface

# OpenAI Embeddings
OPENAI_EMBEDDING_MODEL_NAME=text-embedding-3-small
OPENAI_EMBEDDING_DIMENSION=1536

# HuggingFace Embeddings (local, unlimited, free)
HUGGINGFACE_EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
HUGGINGFACE_EMBEDDING_MODEL_DIMENSION=384

# Ollama Embeddings (for local models)
# OLLAMA_EMBEDDING_MODEL_NAME=nomic-embed-text
# OLLAMA_EMBEDDING_MODEL_DIMENSION=768

# -----------------------------------------------------------------------------
# Data Processing Configuration
# -----------------------------------------------------------------------------
# Chunk size for document processing (in tokens)
CHUNK_SIZE=512
# Overlap between chunks (in tokens, ~20% for context preservation)
CHUNK_OVERLAP=100

# -----------------------------------------------------------------------------
# Core Workflow Configuration
# -----------------------------------------------------------------------------

# Summary and Memory Management
# Number of messages before triggering summarization
SUMMARY_THRESHOLD=15
# Number of recent messages to keep after summarization
MESSAGES_TO_KEEP_AFTER_SUMMARY=3

# Workflow Safety
# Maximum iterations to prevent infinite loops
MAX_ITERATIONS=100

# LLM Temperature Settings
# Main agent: balanced creativity and accuracy
AGENT_TEMPERATURE=0.3
# Summary agent: low temperature for factual consistency
SUMMARY_LLM_TEMPERATURE=0.1
# Document analyst: low temperature for analytical precision
DOCUMENT_ANALYST_TEMPERATURE=0.2
# Document synthesizer: higher temperature for creative synthesis
DOCUMENT_SYNTHESIZER_TEMPERATURE=0.4

# RAG Configuration
# Number of top results to retrieve from knowledge base
RAG_TOP_K=5
# Minimum similarity score threshold (0.0 = no threshold)
RAG_SIMILARITY_THRESHOLD=0.0

# -----------------------------------------------------------------------------
# ArXiv Configuration
# -----------------------------------------------------------------------------
ARXIV_DEFAULT_QUERY=What are the latest advancements in face analysis
ARXIV_MAX_RESULTS=10
ARXIV_SORT_BY=SubmittedDate
ARXIV_SORT_ORDER=Descending
ARXIV_DOWNLOAD_DELAY_SECONDS=3

# -----------------------------------------------------------------------------
# API Configuration
# -----------------------------------------------------------------------------
# CORS allowed origins (use specific domains in production)
# Example for production: ALLOWED_ORIGINS=["https://yourdomain.com", "https://app.yourdomain.com"]
ALLOWED_ORIGINS=["*"]

# -----------------------------------------------------------------------------
# Storage Paths (Optional - defaults are usually fine)
# -----------------------------------------------------------------------------
# SQLite database path for LangGraph checkpoints
# SQLITE_DB_PATH=./data/checkpoints.sqlite

# ChromaDB path for vector storage
# CHROMA_DB_PATH=./data/chroma_db

# ChromaDB collection name
# CHROMA_COLLECTION_NAME=arxiv_chunks

# Data directory
# DATA_DIR=./data

# Evaluation dataset path
# EVALUATION_DATASET_PATH=./data/evaluation/rag_eval_dataset.json
